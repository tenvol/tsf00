{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-class_classification_of_handwritten_digits.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL",
        "266KQvZoMxMv",
        "6sfw3LH0Oycm"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tenvol/tsf00/blob/master/multi_class_classification_of_handwritten_digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "JndnmDMp66FL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2017 Google LLC."
      ]
    },
    {
      "metadata": {
        "id": "hMqWDc_m6rUC",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPa95uXvcpcn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classifying Handwritten Digits with Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "Fdpn8b90u8Tp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![img](https://www.tensorflow.org/versions/r0.11/images/MNIST.png)"
      ]
    },
    {
      "metadata": {
        "id": "c7HLCm66Cs2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Learning Objectives:**\n",
        "  * Train both a linear model and a neural network to classify handwritten digits from the classic [MNIST](http://yann.lecun.com/exdb/mnist/) data set\n",
        "  * Compare the performance of the linear and neural network classification models\n",
        "  * Visualize the weights of a neural-network hidden layer"
      ]
    },
    {
      "metadata": {
        "id": "HSEh-gNdu8T0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our goal is to map each input image to the correct numeric digit. We will create a NN with a few hidden layers and a Softmax layer at the top to select the winning class."
      ]
    },
    {
      "metadata": {
        "id": "2NMdE1b-7UIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "First, let's download the data set, import TensorFlow and other utilities, and load the data into a *pandas* `DataFrame`. Note that this data is a sample of the original MNIST training data; we've taken 20000 rows at random."
      ]
    },
    {
      "metadata": {
        "id": "4LJ4SD8BWHeh",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "88d4b9f0-a907-4e1a-a39e-7cfc8f2704a4"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "\n",
        "mnist_dataframe = pd.read_csv(\n",
        "  \"https://download.mlcc.google.com/mledu-datasets/mnist_train_small.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "# Use just the first 10,000 records for training/validation.\n",
        "mnist_dataframe = mnist_dataframe.head(10000)\n",
        "\n",
        "mnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
        "mnist_dataframe.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5102</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7103</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7024</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3158</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1    2    3    4    5    6    7    8    9   ...   775  776  777  \\\n",
              "5102    2    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "7103    1    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "7024    1    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "110     2    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "3158    8    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "\n",
              "      778  779  780  781  782  783  784  \n",
              "5102    0    0    0    0    0    0    0  \n",
              "7103    0    0    0    0    0    0    0  \n",
              "7024    0    0    0    0    0    0    0  \n",
              "110     0    0    0    0    0    0    0  \n",
              "3158    0    0    0    0    0    0    0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "gC_j3j7V9wwl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mndf = mnist_dataframe.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LJWAETrU91rV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a322ec78-a85b-4fed-e17c-2e56eafb1cb2"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 785)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "_6fNG_XG93zE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "7fe78ceb-25f7-4f5c-9b5d-6959dfb1a625"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.head(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5102</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7103</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7024</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1    2    3    4    5    6    7    8    9   ...   775  776  777  \\\n",
              "5102    2    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "7103    1    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "7024    1    0    0    0    0    0    0    0    0    0 ...     0    0    0   \n",
              "\n",
              "      778  779  780  781  782  783  784  \n",
              "5102    0    0    0    0    0    0    0  \n",
              "7103    0    0    0    0    0    0    0  \n",
              "7024    0    0    0    0    0    0    0  \n",
              "\n",
              "[3 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "BU-6_KTb96hK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2a7db1bd-7a50-4c00-b2a0-cfac1956b323"
      },
      "cell_type": "code",
      "source": [
        "mndf.groupby(0).apply(lambda x: x[0].count())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0\n",
              "0     979\n",
              "1    1107\n",
              "2    1001\n",
              "3     981\n",
              "4     943\n",
              "5     894\n",
              "6    1021\n",
              "7    1085\n",
              "8     973\n",
              "9    1016\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "L00-M8Vh-r2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "dd084003-df19-4415-8803-57dafbc3e764"
      },
      "cell_type": "code",
      "source": [
        "mndf.groupby(0).apply(lambda x: x[9].count())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0\n",
              "0     979\n",
              "1    1107\n",
              "2    1001\n",
              "3     981\n",
              "4     943\n",
              "5     894\n",
              "6    1021\n",
              "7    1085\n",
              "8     973\n",
              "9    1016\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "-3tdNrN4-yha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "47b67246-1025-4ede-8d94-9c5d1540e52a"
      },
      "cell_type": "code",
      "source": [
        "mndf.groupby(0).agg('count')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>...</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>...</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>...</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>...</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>...</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>...</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>...</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>...</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>...</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>...</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    1     2     3     4     5     6     7     8     9     10   ...    775  \\\n",
              "0                                                              ...          \n",
              "0   979   979   979   979   979   979   979   979   979   979  ...    979   \n",
              "1  1107  1107  1107  1107  1107  1107  1107  1107  1107  1107  ...   1107   \n",
              "2  1001  1001  1001  1001  1001  1001  1001  1001  1001  1001  ...   1001   \n",
              "3   981   981   981   981   981   981   981   981   981   981  ...    981   \n",
              "4   943   943   943   943   943   943   943   943   943   943  ...    943   \n",
              "5   894   894   894   894   894   894   894   894   894   894  ...    894   \n",
              "6  1021  1021  1021  1021  1021  1021  1021  1021  1021  1021  ...   1021   \n",
              "7  1085  1085  1085  1085  1085  1085  1085  1085  1085  1085  ...   1085   \n",
              "8   973   973   973   973   973   973   973   973   973   973  ...    973   \n",
              "9  1016  1016  1016  1016  1016  1016  1016  1016  1016  1016  ...   1016   \n",
              "\n",
              "    776   777   778   779   780   781   782   783   784  \n",
              "0                                                        \n",
              "0   979   979   979   979   979   979   979   979   979  \n",
              "1  1107  1107  1107  1107  1107  1107  1107  1107  1107  \n",
              "2  1001  1001  1001  1001  1001  1001  1001  1001  1001  \n",
              "3   981   981   981   981   981   981   981   981   981  \n",
              "4   943   943   943   943   943   943   943   943   943  \n",
              "5   894   894   894   894   894   894   894   894   894  \n",
              "6  1021  1021  1021  1021  1021  1021  1021  1021  1021  \n",
              "7  1085  1085  1085  1085  1085  1085  1085  1085  1085  \n",
              "8   973   973   973   973   973   973   973   973   973  \n",
              "9  1016  1016  1016  1016  1016  1016  1016  1016  1016  \n",
              "\n",
              "[10 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "P1-awioT-7ZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "408a8db1-1f60-466f-eff2-5b17096100e6"
      },
      "cell_type": "code",
      "source": [
        "mndf.groupby(0).agg({9: 'count', 700: 'count'})"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>9</th>\n",
              "      <th>700</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>979</td>\n",
              "      <td>979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1107</td>\n",
              "      <td>1107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1001</td>\n",
              "      <td>1001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>981</td>\n",
              "      <td>981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>943</td>\n",
              "      <td>943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>894</td>\n",
              "      <td>894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1021</td>\n",
              "      <td>1021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1085</td>\n",
              "      <td>1085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>973</td>\n",
              "      <td>973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1016</td>\n",
              "      <td>1016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    9     700\n",
              "0            \n",
              "0   979   979\n",
              "1  1107  1107\n",
              "2  1001  1001\n",
              "3   981   981\n",
              "4   943   943\n",
              "5   894   894\n",
              "6  1021  1021\n",
              "7  1085  1085\n",
              "8   973   973\n",
              "9  1016  1016"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "ZVHS5zU--e-j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "79a79398-80ed-4696-e7e3-0806f2f0d5e5"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.groupby(0)[0].hist(bins=10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0\n",
              "0    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "3    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "4    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "5    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "6    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "7    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "8    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "9    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFKCAYAAAA0WNeQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0e465e4dd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "m9_QtHI4AHrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "dff0c352-a947-463c-9903-80bff7e82665"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.groupby(0).agg({784: 'count'})"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1016</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    784\n",
              "0      \n",
              "0   979\n",
              "1  1107\n",
              "2  1001\n",
              "3   981\n",
              "4   943\n",
              "5   894\n",
              "6  1021\n",
              "7  1085\n",
              "8   973\n",
              "9  1016"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "Us9DPw6gBReZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c3a666a-66de-4aa1-c560-af1f8bef2876"
      },
      "cell_type": "code",
      "source": [
        "type(mnist_dataframe.groupby(0).agg('count'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "pVopUSR2EL50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ba0fb8ec-3ed1-4860-c7dd-73bf32ef73d7"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.groupby(0).agg('count')[96]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0\n",
              "0     979\n",
              "1    1107\n",
              "2    1001\n",
              "3     981\n",
              "4     943\n",
              "5     894\n",
              "6    1021\n",
              "7    1085\n",
              "8     973\n",
              "9    1016\n",
              "Name: 96, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "85BIEKNgIvIf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "81587ae9-f6df-438a-93cd-fac6c8e21a90"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.groupby(0)[9].hist(bins=10)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0\n",
              "0    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "3    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "4    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "5    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "6    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "7    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "8    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "9    AxesSubplot(0.125,0.125;0.775x0.755)\n",
              "Name: 9, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFKCAYAAADScRzUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0e43d62650>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Pi4ggs46BZCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b818ae0b-aa0a-48da-eb9b-ff8bf1b511a4"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.groupby(0)[9].agg('count').plot(kind='bar')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0e437c8f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFUCAYAAADrrX8/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0e438b2410>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "MdTlM4chLo8Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "cda3ce7b-992b-4043-c212-c13c3a69b9f1"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.groupby(0)[9].agg('count')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0\n",
              "0     979\n",
              "1    1107\n",
              "2    1001\n",
              "3     981\n",
              "4     943\n",
              "5     894\n",
              "6    1021\n",
              "7    1085\n",
              "8     973\n",
              "9    1016\n",
              "Name: 9, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "iR4gF8S1LsXA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kg0-25p2mOi0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each row represents one labeled example. Column 0 represents the label that a human rater has assigned for one handwritten digit. For example, if Column 0 contains '6', then a human rater interpreted the handwritten character as the digit '6'.  The ten digits 0-9 are each represented, with a unique class label for each possible digit. Thus, this is a multi-class classification problem with 10 classes."
      ]
    },
    {
      "metadata": {
        "id": "PQ7vuOwRCsZ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![img](https://www.tensorflow.org/versions/r0.11/images/MNIST-Matrix.png)"
      ]
    },
    {
      "metadata": {
        "id": "dghlqJPIu8UM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Columns 1 through 784 contain the feature values, one per pixel for the 28×28=784 pixel values. The pixel values are on a gray scale in which 0 represents white, 255 represents black, and values between 0 and 255 represent shades of gray. Most of the pixel values are 0; you may want to take a minute to confirm that they aren't all 0.  For example, adjust the following text block to print out the values in column 72."
      ]
    },
    {
      "metadata": {
        "id": "2ZkrL5MCqiJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "5be9b869-9e06-400c-fa0c-3d610455152a"
      },
      "cell_type": "code",
      "source": [
        "mnist_dataframe.loc[:, 72:72]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>72</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9404</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6090</th>\n",
              "      <td>170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8417</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4937</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5990</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>613</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5652</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2385</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>878</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       72\n",
              "9404    0\n",
              "6090  170\n",
              "8417    0\n",
              "4937    0\n",
              "5990    0\n",
              "...   ...\n",
              "613     0\n",
              "5652    0\n",
              "1989    0\n",
              "2385    0\n",
              "878     0\n",
              "\n",
              "[10000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "ep4Smv_o-JIz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vLNg2VxqhUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let's parse out the labels and features and look at a few examples. Note the use of `loc` which allows us to pull out columns based on original location, since we don't have a header row in this data set."
      ]
    },
    {
      "metadata": {
        "id": "JfFWWvMWDFrR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse_labels_and_features(dataset):\n",
        "  \"\"\"Extracts labels and features.\n",
        "  \n",
        "  This is a good place to scale or transform the features if needed.\n",
        "  \n",
        "  Args:\n",
        "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
        "      monochrome pixel values on the remaining columns, in row major order.\n",
        "  Returns:\n",
        "    A `tuple` `(labels, features)`:\n",
        "      labels: A Pandas `Series`.\n",
        "      features: A Pandas `DataFrame`.\n",
        "  \"\"\"\n",
        "  labels = dataset[0]\n",
        "\n",
        "  # DataFrame.loc index ranges are inclusive at both ends.\n",
        "  features = dataset.loc[:,1:784]\n",
        "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
        "  features = features / 255\n",
        "\n",
        "  return labels, features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GcoWUz6cPejV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFY_-7vZu8UU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "d847761a-fb23-4810-dc42-a0051442e97d"
      },
      "cell_type": "code",
      "source": [
        "training_targets, training_examples = parse_labels_and_features(mnist_dataframe[:7500])\n",
        "training_examples.describe()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>...</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>7500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1      2      3      4      5      6      7      8      9      10   \\\n",
              "count 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0   \n",
              "mean     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "       ...      775    776    777    778    779    780    781    782    783  \\\n",
              "count  ...   7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0 7500.0   \n",
              "mean   ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max    ...      1.0    1.0    0.3    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "         784  \n",
              "count 7500.0  \n",
              "mean     0.0  \n",
              "std      0.0  \n",
              "min      0.0  \n",
              "25%      0.0  \n",
              "50%      0.0  \n",
              "75%      0.0  \n",
              "max      0.0  \n",
              "\n",
              "[8 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "mNcT3Al1PnQ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-Vgg-1zu8Ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "f418fab9-aa0e-47c3-9306-8f5b058b3f54"
      },
      "cell_type": "code",
      "source": [
        "validation_targets, validation_examples = parse_labels_and_features(mnist_dataframe[7500:10000])\n",
        "validation_examples.describe()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "      <th>784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1      2      3      4      5      6      7      8      9      10   \\\n",
              "count 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0   \n",
              "mean     0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "\n",
              "       ...      775    776    777    778    779    780    781    782    783  \\\n",
              "count  ...   2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0 2500.0   \n",
              "mean   ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "std    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "min    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "25%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "50%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "75%    ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
              "max    ...      1.0    1.0    0.8    0.2    1.0    0.2    0.0    0.0    0.0   \n",
              "\n",
              "         784  \n",
              "count 2500.0  \n",
              "mean     0.0  \n",
              "std      0.0  \n",
              "min      0.0  \n",
              "25%      0.0  \n",
              "50%      0.0  \n",
              "75%      0.0  \n",
              "max      0.0  \n",
              "\n",
              "[8 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "wrnAI1v6u8Uh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Show a random example and its corresponding label."
      ]
    },
    {
      "metadata": {
        "id": "s-euVJVtu8Ui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "0b595ef7-e3ff-41d4-a7db-2267be699f6e"
      },
      "cell_type": "code",
      "source": [
        "rand_example = np.random.choice(training_examples.index)\n",
        "_, ax = plt.subplots()\n",
        "ax.matshow(training_examples.loc[rand_example].values.reshape(28, 28))\n",
        "ax.set_title(\"Label: %i\" % training_targets.loc[rand_example])\n",
        "ax.grid(False)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFXCAYAAAAro2x+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0e4247ee90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GxY_G9SrQNZ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "e4de65a0-d9ac-46b1-ea13-048169e29c8f"
      },
      "cell_type": "code",
      "source": [
        "rand_example = np.random.choice(training_examples.index)\n",
        "_, ax = plt.subplots()\n",
        "ax.matshow(training_examples.loc[rand_example].values.reshape(28, 28))\n",
        "ax.set_title(\"Label: %i\" % training_targets.loc[rand_example])\n",
        "ax.grid(False)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFXCAYAAAAro2x+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0e422ea850>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_48rauOvQnQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e42aef78-c5d3-4ef1-b10a-9981fa869364"
      },
      "cell_type": "code",
      "source": [
        "training_examples.loc[rand_example]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1     0.0\n",
              "2     0.0\n",
              "3     0.0\n",
              "4     0.0\n",
              "5     0.0\n",
              "       ..\n",
              "780   0.0\n",
              "781   0.0\n",
              "782   0.0\n",
              "783   0.0\n",
              "784   0.0\n",
              "Name: 5787, Length: 784, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "YeBMIlw5REUx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2873
        },
        "outputId": "d7e6a441-c57d-47e6-a2ec-56fa03705a1f"
      },
      "cell_type": "code",
      "source": [
        "training_examples.loc[rand_example].values.reshape(28,28)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.04313725, 0.2       , 0.2       , 0.2       , 0.2       ,\n",
              "        0.2       , 0.28235294, 0.59607843, 0.83921569, 0.83529412,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.16078431,\n",
              "        0.83529412, 0.98823529, 0.99215686, 0.98823529, 0.99215686,\n",
              "        0.98823529, 0.99215686, 0.98823529, 0.99215686, 0.67058824,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.48235294,\n",
              "        1.        , 0.99215686, 1.        , 0.91372549, 0.71764706,\n",
              "        0.55686275, 0.83921569, 0.99215686, 1.        , 0.19607843,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.08235294, 0.8745098 ,\n",
              "        0.99215686, 0.98823529, 0.35686275, 0.11764706, 0.        ,\n",
              "        0.        , 0.51764706, 0.98823529, 0.6745098 , 0.03921569,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.67843137, 0.99215686,\n",
              "        1.        , 0.6745098 , 0.32156863, 0.        , 0.        ,\n",
              "        0.08235294, 1.        , 0.99215686, 0.16078431, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.83529412, 0.98823529,\n",
              "        0.83529412, 0.03921569, 0.        , 0.        , 0.        ,\n",
              "        0.4       , 0.99215686, 0.98823529, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.24313725, 0.4       ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.63921569, 0.99607843, 0.51372549, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.08235294,\n",
              "        0.8745098 , 0.91372549, 0.11764706, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.44313725,\n",
              "        0.99215686, 0.79607843, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.59607843,\n",
              "        0.98823529, 0.63529412, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.24313725, 0.99607843,\n",
              "        0.99215686, 0.32156863, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.55686275, 0.99215686,\n",
              "        0.67058824, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.04313725, 0.83529412, 0.99607843,\n",
              "        0.35686275, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.36078431, 0.98823529, 0.6745098 ,\n",
              "        0.03921569, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.99607843, 0.99215686, 0.4       ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.16078431, 0.99215686, 0.83137255, 0.07843137,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.63921569, 0.95686275, 0.15686275, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.16078431, 0.95294118, 0.63529412, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.2       , 0.99215686, 0.16078431, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.2       , 0.98823529, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "6wJAZA5YQ5T0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2686
        },
        "outputId": "20d4ef5b-db0c-49bf-c4dc-6e7c8d605fa3"
      },
      "cell_type": "code",
      "source": [
        "training_examples.loc[rand_example].values"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.04313725,\n",
              "       0.2       , 0.2       , 0.2       , 0.2       , 0.2       ,\n",
              "       0.28235294, 0.59607843, 0.83921569, 0.83529412, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.16078431, 0.83529412, 0.98823529, 0.99215686,\n",
              "       0.98823529, 0.99215686, 0.98823529, 0.99215686, 0.98823529,\n",
              "       0.99215686, 0.67058824, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.48235294,\n",
              "       1.        , 0.99215686, 1.        , 0.91372549, 0.71764706,\n",
              "       0.55686275, 0.83921569, 0.99215686, 1.        , 0.19607843,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.08235294, 0.8745098 , 0.99215686, 0.98823529,\n",
              "       0.35686275, 0.11764706, 0.        , 0.        , 0.51764706,\n",
              "       0.98823529, 0.6745098 , 0.03921569, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.67843137,\n",
              "       0.99215686, 1.        , 0.6745098 , 0.32156863, 0.        ,\n",
              "       0.        , 0.08235294, 1.        , 0.99215686, 0.16078431,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.83529412, 0.98823529, 0.83529412,\n",
              "       0.03921569, 0.        , 0.        , 0.        , 0.4       ,\n",
              "       0.99215686, 0.98823529, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.24313725, 0.4       , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.63921569, 0.99607843, 0.51372549,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.08235294,\n",
              "       0.8745098 , 0.91372549, 0.11764706, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.44313725, 0.99215686, 0.79607843,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.59607843, 0.98823529, 0.63529412, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.24313725, 0.99607843, 0.99215686,\n",
              "       0.32156863, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.55686275, 0.99215686, 0.67058824, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.04313725, 0.83529412, 0.99607843,\n",
              "       0.35686275, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.36078431, 0.98823529, 0.6745098 , 0.03921569, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.99607843, 0.99215686,\n",
              "       0.4       , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.16078431, 0.99215686, 0.83137255, 0.07843137, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.63921569, 0.95686275,\n",
              "       0.15686275, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.16078431, 0.95294118, 0.63529412, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.2       , 0.99215686,\n",
              "       0.16078431, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.2       , 0.98823529, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "8v2b5wtCQ7Ql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ScmYX7xdZMXE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1: Build a Linear Model for MNIST\n",
        "\n",
        "First, let's create a baseline model to compare against. The `LinearClassifier` provides a set of *k* one-vs-all classifiers, one for each of the *k* classes.\n",
        "\n",
        "You'll notice that in addition to reporting accuracy, and plotting Log Loss over time, we also display a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix).  The confusion matrix shows which classes were misclassified as other classes. Which digits get confused for each other?\n",
        "\n",
        "Also note that we track the model's error using the `log_loss` function. This should not be confused with the loss function internal to `LinearClassifier` that is used for training."
      ]
    },
    {
      "metadata": {
        "id": "cpoVC4TSdw5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def construct_feature_columns():\n",
        "  \"\"\"Construct the TensorFlow Feature Columns.\n",
        "\n",
        "  Returns:\n",
        "    A set of feature columns\n",
        "  \"\"\" \n",
        "  \n",
        "  # There are 784 pixels in each image.\n",
        "  return set([tf.feature_column.numeric_column('pixels', shape=784)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kMmL89yGeTfz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we'll make separate input functions for training and for prediction. We'll nest them in `create_training_input_fn()` and `create_predict_input_fn()`, respectively, so we can invoke these functions to return the corresponding `_input_fn`s to pass to our `.train()` and `.predict()` calls."
      ]
    },
    {
      "metadata": {
        "id": "OeS47Bmn5Ms2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_training_input_fn(features, labels, batch_size, num_epochs=None, shuffle=True):\n",
        "  \"\"\"A custom input_fn for sending MNIST data to the estimator for training.\n",
        "\n",
        "  Args:\n",
        "    features: The training features.\n",
        "    labels: The training labels.\n",
        "    batch_size: Batch size to use during training.\n",
        "\n",
        "  Returns:\n",
        "    A function that returns batches of training features and labels during\n",
        "    training.\n",
        "  \"\"\"\n",
        "  def _input_fn(num_epochs=None, shuffle=True):\n",
        "    # Input pipelines are reset with each call to .train(). To ensure model\n",
        "    # gets a good sampling of data, even when number of steps is small, we \n",
        "    # shuffle all the data before creating the Dataset object\n",
        "    idx = np.random.permutation(features.index)\n",
        "    raw_features = {\"pixels\":features.reindex(idx)}\n",
        "    raw_targets = np.array(labels[idx])\n",
        "   \n",
        "    ds = Dataset.from_tensor_slices((raw_features,raw_targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
        "    \n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(10000)\n",
        "    \n",
        "    # Return the next batch of data.\n",
        "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
        "    return feature_batch, label_batch\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8zoGWAoohrwS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_predict_input_fn(features, labels, batch_size):\n",
        "  \"\"\"A custom input_fn for sending mnist data to the estimator for predictions.\n",
        "\n",
        "  Args:\n",
        "    features: The features to base predictions on.\n",
        "    labels: The labels of the prediction examples.\n",
        "\n",
        "  Returns:\n",
        "    A function that returns features and labels for predictions.\n",
        "  \"\"\"\n",
        "  def _input_fn():\n",
        "    raw_features = {\"pixels\": features.values}\n",
        "    raw_targets = np.array(labels)\n",
        "    \n",
        "    ds = Dataset.from_tensor_slices((raw_features, raw_targets)) # warning: 2GB limit\n",
        "    ds = ds.batch(batch_size)\n",
        "    \n",
        "        \n",
        "    # Return the next batch of data.\n",
        "    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n",
        "    return feature_batch, label_batch\n",
        "\n",
        "  return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6DjSLZMu8Um",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_linear_classification_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a linear classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, and a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: An `int`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `LinearClassifier` object.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "\n",
        "  steps_per_period = steps / periods  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create a LinearClassifier object.\n",
        "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.LinearClassifier(\n",
        "      feature_columns=construct_feature_columns(),\n",
        "      n_classes=10,\n",
        "      optimizer=my_optimizer,\n",
        "      config=tf.estimator.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "  \n",
        "    # Take a break and compute probabilities.\n",
        "    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "    training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
        "        \n",
        "    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "    validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
        "    \n",
        "    # Compute training and validation errors.\n",
        "    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_errors.append(training_log_loss)\n",
        "    validation_errors.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class).\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ItHIUyv2u8Ur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Spend 5 minutes seeing how well you can do on accuracy with a linear model of this form. For this exercise, limit yourself to experimenting with the hyperparameters for batch size, learning rate and steps.**\n",
        "\n",
        "Stop if you get anything above about 0.9 accuracy."
      ]
    },
    {
      "metadata": {
        "id": "yaiIhIQqu8Uv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = train_linear_classification_model(\n",
        "             learning_rate=0.02,\n",
        "             steps=100,\n",
        "             batch_size=10,\n",
        "             training_examples=training_examples,\n",
        "             training_targets=training_targets,\n",
        "             validation_examples=validation_examples,\n",
        "             validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "266KQvZoMxMv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "Click below for one possible solution."
      ]
    },
    {
      "metadata": {
        "id": "lRWcn24DM3qa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is a set of parameters that should attain roughly 0.9 accuracy."
      ]
    },
    {
      "metadata": {
        "id": "TGlBMrUoM1K_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_ = train_linear_classification_model(\n",
        "    learning_rate=0.03,\n",
        "    steps=1000,\n",
        "    batch_size=30,\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mk095OfpPdOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2: Replace the Linear Classifier with a Neural Network\n",
        "\n",
        "**Replace the LinearClassifier above with a [`DNNClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) and find a parameter combination that gives 0.95 or better accuracy.**\n",
        "\n",
        "You may wish to experiment with additional regularization methods, such as dropout. These additional regularization methods are documented in the comments for the `DNNClassifier` class."
      ]
    },
    {
      "metadata": {
        "id": "rm8P_Ttwu8U4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# YOUR CODE HERE: Replace the linear classifier with a neural network.\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TOfmiSvqu8U9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once you have a good model, double check that you didn't overfit the validation set by evaluating on the test data that we'll load below.\n"
      ]
    },
    {
      "metadata": {
        "id": "evlB5ubzu8VJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_test_dataframe = pd.read_csv(\n",
        "  \"https://download.mlcc.google.com/mledu-datasets/mnist_test.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
        "test_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDuLd2Hcu8VL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# YOUR CODE HERE: Calculate accuracy on the test set.\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6sfw3LH0Oycm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Solution\n",
        "\n",
        "Click below for a possible solution."
      ]
    },
    {
      "metadata": {
        "id": "XatDGFKEO374",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code below is almost identical to the original `LinearClassifer` training code, with the exception of the NN-specific configuration, such as the hyperparameter for hidden units."
      ]
    },
    {
      "metadata": {
        "id": "kdNTx8jkPQUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_nn_classification_model(\n",
        "    learning_rate,\n",
        "    steps,\n",
        "    batch_size,\n",
        "    hidden_units,\n",
        "    training_examples,\n",
        "    training_targets,\n",
        "    validation_examples,\n",
        "    validation_targets):\n",
        "  \"\"\"Trains a neural network classification model for the MNIST digits dataset.\n",
        "  \n",
        "  In addition to training, this function also prints training progress information,\n",
        "  a plot of the training and validation loss over time, as well as a confusion\n",
        "  matrix.\n",
        "  \n",
        "  Args:\n",
        "    learning_rate: An `int`, the learning rate to use.\n",
        "    steps: A non-zero `int`, the total number of training steps. A training step\n",
        "      consists of a forward and backward pass using a single batch.\n",
        "    batch_size: A non-zero `int`, the batch size.\n",
        "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
        "    training_examples: A `DataFrame` containing the training features.\n",
        "    training_targets: A `DataFrame` containing the training labels.\n",
        "    validation_examples: A `DataFrame` containing the validation features.\n",
        "    validation_targets: A `DataFrame` containing the validation labels.\n",
        "      \n",
        "  Returns:\n",
        "    The trained `DNNClassifier` object.\n",
        "  \"\"\"\n",
        "\n",
        "  periods = 10\n",
        "  # Caution: input pipelines are reset with each call to train. \n",
        "  # If the number of steps is small, your model may never see most of the data.  \n",
        "  # So with multiple `.train` calls like this you may want to control the length \n",
        "  # of training with num_epochs passed to the input_fn. Or, you can do a really-big shuffle, \n",
        "  # or since it's in-memory data, shuffle all the data in the `input_fn`.\n",
        "  steps_per_period = steps / periods  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create the input functions.\n",
        "  predict_training_input_fn = create_predict_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  predict_validation_input_fn = create_predict_input_fn(\n",
        "    validation_examples, validation_targets, batch_size)\n",
        "  training_input_fn = create_training_input_fn(\n",
        "    training_examples, training_targets, batch_size)\n",
        "  \n",
        "  # Create feature columns.\n",
        "  feature_columns = [tf.feature_column.numeric_column('pixels', shape=784)]\n",
        "\n",
        "  # Create a DNNClassifier object.\n",
        "  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
        "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
        "  classifier = tf.estimator.DNNClassifier(\n",
        "      feature_columns=feature_columns,\n",
        "      n_classes=10,\n",
        "      hidden_units=hidden_units,\n",
        "      optimizer=my_optimizer,\n",
        "      config=tf.contrib.learn.RunConfig(keep_checkpoint_max=1)\n",
        "  )\n",
        "\n",
        "  # Train the model, but do so inside a loop so that we can periodically assess\n",
        "  # loss metrics.\n",
        "  print(\"Training model...\")\n",
        "  print(\"LogLoss error (on validation data):\")\n",
        "  training_errors = []\n",
        "  validation_errors = []\n",
        "  for period in range (0, periods):\n",
        "    # Train the model, starting from the prior state.\n",
        "    classifier.train(\n",
        "        input_fn=training_input_fn,\n",
        "        steps=steps_per_period\n",
        "    )\n",
        "  \n",
        "    # Take a break and compute probabilities.\n",
        "    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
        "    training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
        "    training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
        "    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
        "        \n",
        "    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
        "    validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
        "    validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
        "    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n",
        "    \n",
        "    # Compute training and validation errors.\n",
        "    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
        "    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
        "    # Occasionally print the current loss.\n",
        "    print(\"  period %02d : %0.2f\" % (period, validation_log_loss))\n",
        "    # Add the loss metrics from this period to our list.\n",
        "    training_errors.append(training_log_loss)\n",
        "    validation_errors.append(validation_log_loss)\n",
        "  print(\"Model training finished.\")\n",
        "  # Remove event files to save disk space.\n",
        "  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
        "  \n",
        "  # Calculate final predictions (not probabilities, as above).\n",
        "  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
        "  final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
        "  \n",
        "  \n",
        "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
        "  print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
        "\n",
        "  # Output a graph of loss metrics over periods.\n",
        "  plt.ylabel(\"LogLoss\")\n",
        "  plt.xlabel(\"Periods\")\n",
        "  plt.title(\"LogLoss vs. Periods\")\n",
        "  plt.plot(training_errors, label=\"training\")\n",
        "  plt.plot(validation_errors, label=\"validation\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  # Output a plot of the confusion matrix.\n",
        "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
        "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
        "  # in each class).\n",
        "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
        "  ax.set_aspect(1)\n",
        "  plt.title(\"Confusion matrix\")\n",
        "  plt.ylabel(\"True label\")\n",
        "  plt.xlabel(\"Predicted label\")\n",
        "  plt.show()\n",
        "\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZfzsTYGPPU8I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classifier = train_nn_classification_model(\n",
        "    learning_rate=0.05,\n",
        "    steps=1000,\n",
        "    batch_size=30,\n",
        "    hidden_units=[100, 100],\n",
        "    training_examples=training_examples,\n",
        "    training_targets=training_targets,\n",
        "    validation_examples=validation_examples,\n",
        "    validation_targets=validation_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qXvrOgtUR-zD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we verify the accuracy on the test set."
      ]
    },
    {
      "metadata": {
        "id": "scQNpDePSFjt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_test_dataframe = pd.read_csv(\n",
        "  \"https://download.mlcc.google.com/mledu-datasets/mnist_test.csv\",\n",
        "  sep=\",\",\n",
        "  header=None)\n",
        "\n",
        "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
        "test_examples.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EVaWpWKvSHmu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predict_test_input_fn = create_predict_input_fn(\n",
        "    test_examples, test_targets, batch_size=100)\n",
        "\n",
        "test_predictions = classifier.predict(input_fn=predict_test_input_fn)\n",
        "test_predictions = np.array([item['class_ids'][0] for item in test_predictions])\n",
        "  \n",
        "accuracy = metrics.accuracy_score(test_targets, test_predictions)\n",
        "print(\"Accuracy on test data: %0.2f\" % accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WX2mQBAEcisO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 3: Visualize the weights of the first hidden layer.\n",
        "\n",
        "Let's take a few minutes to dig into our neural network and see what it has learned by accessing the `weights_` attribute of our model.\n",
        "\n",
        "The input layer of our model has `784` weights corresponding to the `28×28` pixel input images. The first hidden layer will have `784×N` weights where `N` is the number of nodes in that layer. We can turn those weights back into `28×28` images by *reshaping* each of the `N` `1×784` arrays of weights into `N` arrays of size `28×28`.\n",
        "\n",
        "Run the following cell to plot the weights. Note that this cell requires that a `DNNClassifier` called \"classifier\" has already been trained."
      ]
    },
    {
      "metadata": {
        "id": "eUC0Z8nbafgG",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(classifier.get_variable_names())\n",
        "\n",
        "weights0 = classifier.get_variable_value(\"dnn/hiddenlayer_0/kernel\")\n",
        "\n",
        "print(\"weights0 shape:\", weights0.shape)\n",
        "\n",
        "num_nodes = weights0.shape[1]\n",
        "num_rows = int(math.ceil(num_nodes / 10.0))\n",
        "fig, axes = plt.subplots(num_rows, 10, figsize=(20, 2 * num_rows))\n",
        "for coef, ax in zip(weights0.T, axes.ravel()):\n",
        "    # Weights in coef is reshaped from 1x784 to 28x28.\n",
        "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.pink)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kL8MEhNgrx9N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first hidden layer of the neural network should be modeling some pretty low level features, so visualizing the weights will probably just show some fuzzy blobs or possibly a few parts of digits.  You may also see some neurons that are essentially noise -- these are either unconverged or they are being ignored by higher layers.\n",
        "\n",
        "It can be interesting to stop training at different numbers of iterations and see the effect.\n",
        "\n",
        "**Train the classifier for 10, 100 and respectively 1000 steps. Then run this visualization again.**\n",
        "\n",
        "What differences do you see visually for the different levels of convergence?"
      ]
    }
  ]
}